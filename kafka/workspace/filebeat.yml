filebeat.inputs:
  - type: log
    multiline.match: after
    multiline.negate: true
    multiline.pattern: '\[(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2},\d{3})\]'
    multiline.type: pattern
    paths:
      - /root/PySpark/workspace/logs/*/*.log
      - /root/PySpark/workspace/logs/*/*/*.log
      - /root/PySpark/workspace/logs/*/*/*/*.log
      #- /var/lib/airflow/logs/scheduler/*/*.log

# ============================== Filebeat modules ==============================

filebeat.config.modules:
  # Glob pattern for configuration loading
  path: ${path.config}/modules.d/*.yml

  # Set to true to enable config reloading
  reload.enabled: true

  # Period on which files under path should be checked for changes
  reload.period: 10s

#output.console:
#  pretty: true
#output.file:
#  path: "/root/test_airflow_log"
#  filename: test.txt

output.kafka:
  hosts: ["kafka1:19091", "kafka2:19092", "kafka3:19093"]
  topic: 'airflow-log'
  partition.round_robin:
    reachable_only: false

  required_acks: 1
  compression: gzip
  max_message_bytes: 1000000

