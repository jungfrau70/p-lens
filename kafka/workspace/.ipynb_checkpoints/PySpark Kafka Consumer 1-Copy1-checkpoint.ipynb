{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "453a5c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fafb6748f28>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8032fb",
   "metadata": {},
   "source": [
    "# Python KafkaConsumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9132c4e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'KafkaUtils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2e6103cd3ff1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# from pyspark.streaming.kafka import KafkaUtils # this is the problem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstreaming\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKafkaUtils\u001b[0m \u001b[0;31m# this is the problem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'KafkaUtils'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row, SparkSession\n",
    "# from pyspark.streaming.kafka import KafkaUtils # this is the problem\n",
    "from pyspark.streaming import KafkaUtils # this is the problem\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b106e5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row, SparkSession\n",
    "from pyspark.streaming.kafka import KafkaUtils # this is the problem\n",
    "import json\n",
    "\n",
    "\n",
    "outputPath = 'C:/Users/Admin/Desktop/kafka_project/checkpoint_01'\n",
    "\n",
    "\n",
    "def getSparkSessionInstance(sparkConf):\n",
    "    if ('sparkSessionSingletonInstance' not in globals()):\n",
    "        globals()['sparkSessionSingletonInstance'] = SparkSession\\\n",
    "            .builder\\\n",
    "            .config(conf=sparkConf)\\\n",
    "            .getOrCreate()\n",
    "    return globals()['sparkSessionSingletonInstance']\n",
    "\n",
    "#-------------------------------------------------\n",
    "# What I want to do per each RDD...\n",
    "#-------------------------------------------------\n",
    "def process(time, rdd):\n",
    "\n",
    "    print(\"===========-----> %s <-----===========\" % str(time))\n",
    "\n",
    "    try:\n",
    "        spark = getSparkSessionInstance(rdd.context.getConf())\n",
    "\n",
    "        rowRdd = rdd.map(lambda w: Row(branch=w['branch'],\n",
    "                                       currency=w['currency'],\n",
    "                                       amount=w['amount']))\n",
    "                                       \n",
    "        testDataFrame = spark.createDataFrame(rowRdd)\n",
    "\n",
    "        testDataFrame.createOrReplaceTempView(\"treasury_stream\")\n",
    "\n",
    "        sql_query = get_sql_query()\n",
    "        testResultDataFrame = spark.sql(sql_query)\n",
    "        testResultDataFrame.show(n=5)\n",
    "\n",
    "        # Insert into DB\n",
    "        try:\n",
    "            testResultDataFrame.write \\\n",
    "                .format(\"jdbc\") \\\n",
    "                .mode(\"append\") \\\n",
    "                .option(\"driver\", 'org.postgresql.Driver') \\\n",
    "                .option(\"url\", \"jdbc:postgresql://myhabrtest.cuyficqfa1h0.ap-south-1.rds.amazonaws.com:5432/habrDB\") \\\n",
    "                .option(\"dbtable\", \"transaction_flow\") \\\n",
    "                .option(\"user\", \"habr\") \\\n",
    "                .option(\"password\", \"habr12345\") \\\n",
    "                .save()\n",
    "        except Exception as e:\n",
    "            print(\"--> Opps! It seems an Errrorrr with DB working!\", e)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"--> Opps! Is seems an Error!!!\", e)\n",
    "\n",
    "#-------------------------------------------------\n",
    "# General function\n",
    "#-------------------------------------------------\n",
    "def createContext():\n",
    "\n",
    "    sc = SparkContext(appName=\"PythonStreamingKafkaTransaction\")\n",
    "    sc.setLogLevel(\"ERROR\")\n",
    "    \n",
    "    ssc = StreamingContext(sc, 2)\n",
    "\n",
    "    broker_list, topic = sys.argv[1:]\n",
    "\n",
    "    try:\n",
    "        directKafkaStream = KafkaUtils.createDirectStream(ssc,\n",
    "                                        [topic],\n",
    "                                        {\"metadata.broker.list\": broker_list})\n",
    "    except:\n",
    "        raise ConnectionError(\"Kafka error: Connection refused: \\\n",
    "                            broker_list={} topic={}\".format(broker_list, topic))\n",
    "\n",
    "    parsed_lines = directKafkaStream.map(lambda v: json.loads(v[1]))\n",
    "\n",
    "    # RDD handling\n",
    "    parsed_lines.foreachRDD(process)\n",
    "\n",
    "    return ssc\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 3:\n",
    "        print(\"Usage: spark_job.py <zk> <topic>\", file=sys.stderr)\n",
    "        exit(-1)\n",
    "        \n",
    "    print(\"--> Creating new context\")\n",
    "    if os.path.exists(outputPath):\n",
    "        shutil.rmtree('outputPath')\n",
    "\n",
    "    ssc = StreamingContext.getOrCreate(outputPath, lambda: createContext())\n",
    "    ssc.start()\n",
    "    ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bad1386",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd6d0d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb9158f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a801d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d95aa57",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark.streaming.kafka'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-772263e53548>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstreaming\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStreamingContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstreaming\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkafka\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKafkaUtils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStructType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStructField\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStringType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIntegerType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark.streaming.kafka'"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "import json\n",
    "from kafka import KafkaConsumer, KafkaClient\n",
    "from pyspark.sql.functions import explode, split\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607c0ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mport pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "import json\n",
    "from kafka import KafkaConsumer, KafkaClient\n",
    "from pyspark.sql.functions import explode, split\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from datetime import date\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "conf.setMaster('mesos://172.20.1.157:5050')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "ssc = StreamingContext(sc, 2)\n",
    "kafkaParams = {'metadata.broker.list': '172.20.1.163:9092', 'auto.offset.reset': 'smallest'} # kafka parameters\n",
    "topics = ['json_topic']\n",
    "kafka_stream = KafkaUtils.createDirectStream(ssc,topics,kafkaParams)\n",
    "parsed = kafka_stream.map(lambda (k,v) : json.loads(v))\n",
    "def function(x):\n",
    "y = x.collect()\n",
    "for d in y :\n",
    "print d\n",
    "rdd_json = json.dumps(d)\n",
    "print rdd_json\n",
    "rdd_json.write.format('org.elasticsearch.spark.sql').mode('append').option('es.index.auto.create','true').option('es.resource', 'sql6/swati').save()\n",
    "\n",
    "parsed.foreachRDD(lambda x :function(x))\n",
    "ssc.start()\n",
    "ssc.awaittermination()\n",
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e495bf63",
   "metadata": {},
   "source": [
    "# PySpark Consumer for Structured Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38ee846",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
